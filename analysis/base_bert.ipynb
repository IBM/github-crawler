{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    " #!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS connecting to Cloudant db github-public-ai-2022\n"
     ]
    }
   ],
   "source": [
    "from utils.cloudant_utils import cloudant_db as db\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['repo', 'release_tag', 'release_date', 'downloads', 'stars', 'watchers', 'forks', 'commits', 'issues', 'total_stars', 'total_forks', 'total_commits', 'contributors', 'total_issues', 'total_closedIssues', 'closedIssues', 'readme', 'readme_size'])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos = [r for r in db.get_query_result({\"type\": \"release\"}, [\"_id\", \"releases\"], limit=10000, raw_result=True)[\"docs\"]]\n",
    "repos[0]['releases'][0].keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "values = [r for release in repos for r in release[\"releases\"]]\n",
    "df = pd.DataFrame(values)\n",
    "df['contributors'] = df['contributors'].apply(lambda x:\n",
    "                                              [i for i in x if i is not None] if isinstance(x, list)\n",
    "                                              else [])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "(44043, 19)"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "df = df[~df['readme'].isnull()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "new_df = df.groupby(\"repo\").agg({\"readme\": list,\n",
    "                                 \"stars\": sum,\n",
    "                                 \"forks\": sum,\n",
    "                                 \"downloads\": sum,\n",
    "                                 \"contributors\": sum})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "readme1 =  new_df.iloc[2]['readme'][0]\n",
    "readme2 = new_df.iloc[2]['readme'][1]\n",
    "# print(len(new_df))\n",
    "# new_df[new_df['readme'].map(lambda d: len(d)) > 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "new_df['600stars']= np.where(new_df['stars'] > 600, 1, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                           readme  \\\nrepo                                                                                \ncatboost/catboost               [<img src=http://storage.mds.yandex.net/get-de...   \nmicrosoft/pai                   [# Open Platform for AI (PAI) ![alt text][logo...   \ndonnemartin/awesome-aws         [<br/>\\n<p align=\"center\">\\n  <img src=\"https:...   \nwhylabs/whylogs                 [# WhyLogs Library\\n[![PyPI version](https://b...   \nzhanghang1989/PyTorch-Encoding  [# PyTorch-Encoding-Layer\\n\\n**Deep TEN: Textu...   \n\n                                600stars  \nrepo                                      \ncatboost/catboost                      1  \nmicrosoft/pai                          1  \ndonnemartin/awesome-aws                1  \nwhylabs/whylogs                        1  \nzhanghang1989/PyTorch-Encoding         1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>readme</th>\n      <th>600stars</th>\n    </tr>\n    <tr>\n      <th>repo</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>catboost/catboost</th>\n      <td>[&lt;img src=http://storage.mds.yandex.net/get-de...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>microsoft/pai</th>\n      <td>[# Open Platform for AI (PAI) ![alt text][logo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>donnemartin/awesome-aws</th>\n      <td>[&lt;br/&gt;\\n&lt;p align=\"center\"&gt;\\n  &lt;img src=\"https:...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>whylabs/whylogs</th>\n      <td>[# WhyLogs Library\\n[![PyPI version](https://b...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>zhanghang1989/PyTorch-Encoding</th>\n      <td>[# PyTorch-Encoding-Layer\\n\\n**Deep TEN: Textu...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.loc[new_df['600stars'] == 1].sample(5)[['readme', '600stars']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " # max_len = 0\n",
    "\n",
    " #for sentences in readme:\n",
    " #    if sentences:\n",
    " #        for sent in sentences:\n",
    " #            if sent:\n",
    " #                input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    " #                max_len = max(max_len, len(input_ids))\n",
    " #print('Max sentence length: ', max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import difflib\n",
    "def diff_calculator(str1, str2):\n",
    "   s = difflib.SequenceMatcher(lambda x : x == '')\n",
    "   s.set_seqs(str1, str2)\n",
    "   i = 1\n",
    "   codes = []\n",
    "   delete = []\n",
    "   replace = {}\n",
    "   insert = []\n",
    "   for (opcode, before_start, before_end, after_start, after_end) in s.get_opcodes():\n",
    "       if opcode == 'equal':\n",
    "           continue\n",
    "       codes.append(opcode)\n",
    "       # print (i, \". %7s '%s :'  ----->  '%s'\" % (opcode, test[0][before_start:before_end], test[1][after_start:after_end]))\n",
    "       if opcode == 'replace':\n",
    "           replace[str1[before_start:before_end]]  = str2[after_start:after_end]\n",
    "       if opcode == 'delete':\n",
    "           delete.append(str1[before_start:before_end])\n",
    "       if opcode == 'insert':\n",
    "           insert.append(str2[after_start:after_end])\n",
    "       i = i + 1\n",
    "   return replace, delete, insert"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 .  haha  --> HAHA\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# def clean(str):\n",
    "#    return re.sub('\\s+', ' ', str) if str is not None else ''\n",
    "# i = 1\n",
    "# replace, _, _ = diff_calculator('haha', \"HAHA\")\n",
    "# for e in replace.keys():\n",
    "#    print(i, '. ', clean(e), ' -->', clean(replace[e]))\n",
    "#    i = i + 1\n",
    "   # print(e, ' -->', (replace[e]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def create_a_sequence(readmeList):\n",
    "   result = []\n",
    "   for i in range(0,len(readmeList)-1):\n",
    "       first = readmeList[i]\n",
    "       second = readmeList[i+1]\n",
    "       _, _, insert = diff_calculator(first, second)\n",
    "       result.append(','.join(insert))\n",
    "   return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# new_df['readme_diff'] = new_df['readme'].apply(lambda x: create_a_sequence(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "result = create_a_sequence(new_df.iloc[2]['readme'])\n",
    "result[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def prepareSequenceForBERT(readmeList):\n",
    "    diffList = create_a_sequence(readmeList)\n",
    "    s = '[CLS]' + \"SEP\".join([str(i) for i in diffList])\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "data": {
      "text/plain": "((4151, 6), (231, 6), (231, 6))"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df_train, df_test = train_test_split(new_df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "df_train.shape, df_val.shape, df_test.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ReadmeDataSet(Dataset):\n",
    "   def __init__(self, df, tokenizer, max_len):\n",
    "      self.df = df\n",
    "      self.tokenizer = tokenizer\n",
    "      self.max_len = max_len\n",
    "\n",
    "   def __len__(self):\n",
    "      return len(self.df)\n",
    "\n",
    "   def __getitem__(self, item):\n",
    "      sequence = prepareSequenceForBERT(self.df.iloc[item]['readme'][0])\n",
    "      # readmes = self.df.iloc[item]['readme']\n",
    "      # for i in range(0,len(readmes)-1):\n",
    "      #   first = readmes[i]\n",
    "      #   second = readmes[i+1]\n",
    "      #   _, _, insert = diff_calculator(first, second)\n",
    "      #   seq = ','.join(insert)\n",
    "      #\n",
    "      # sequence = '[CLS]' + \"SEP\".join([str(i) for i in diffList])\n",
    "\n",
    "      # sequence = \"Hi\"\n",
    "      target = self.df.iloc[item]['600stars']\n",
    "      encoding = self.tokenizer.encode_plus(sequence,\n",
    "                                     None,\n",
    "                                     max_length = self.max_len,\n",
    "                                     truncation=True,\n",
    "                                     add_special_tokens=True,\n",
    "#                                      padding=MAX_LEN,\n",
    "#                                      padding='longest',\n",
    "                                     pad_to_max_length=True,\n",
    "                                     return_token_type_ids=True)\n",
    "\n",
    "      return {\n",
    "      'sequence': sequence,\n",
    "      'input_ids': torch.tensor(encoding.input_ids, dtype=torch.long),\n",
    "      'attention_mask':  torch.tensor(encoding.attention_mask, dtype=torch.long),\n",
    "      'token_type_ids': torch.tensor(encoding.token_type_ids, dtype=torch.long),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "      }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "   ds = ReadmeDataSet(\n",
    "      df = df,\n",
    "      tokenizer=tokenizer,\n",
    "      max_len=max_len\n",
    "   )\n",
    "   return DataLoader(\n",
    "      ds,\n",
    "      batch_size=batch_size,\n",
    "      num_workers=0\n",
    "  )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import BERT Tokenizer and BERT Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                      num_labels=2,\n",
    "                                                      output_attentions= False,\n",
    "                                                      output_hidden_states= False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2 #16\n",
    "MAX_LEN = 100\n",
    "train_data_loader = create_data_loader(df_train, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, bert_tokenizer, MAX_LEN, BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# for d in train_data_loader:\n",
    "#     input_ids = d[\"input_ids\"].to(device)\n",
    "#     attention_mask = d[\"attention_mask\"].to(device)\n",
    "#     targets = d[\"targets\"].to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "#TEST the tokenizer and data loader\n",
    "# sequence = prepareSequenceForBERT(new_df.iloc[2]['readme'])\n",
    "# label = new_df.iloc[2]['600stars']\n",
    "# tokens = bert_tokenizer.encode_plus(\n",
    "#             sequence,\n",
    "#             None,\n",
    "#             max_length= 100,\n",
    "#             truncation=True,\n",
    "#             add_special_tokens=True,\n",
    "# #             pad_to_max_length=True,\n",
    "#             padding = True,\n",
    "#             return_token_type_ids=True\n",
    "#         )\n",
    "# print(f' Sentence: {sequence}')\n",
    "# print(f' Tokens: {tokens}')\n",
    "# print(f' Tokens.token_type_ids: {tokens.token_type_ids}')\n",
    "# print(f' Tokens.input_ids: {len(tokens.input_ids)}')\n",
    "# output = {\n",
    "#       'input_ids': torch.tensor(tokens.input_ids, dtype=torch.long),\n",
    "#       'attention_mask':  torch.tensor(tokens.attention_mask, dtype=torch.long),\n",
    "#       'token_type_ids': torch.tensor(tokens.token_type_ids, dtype=torch.long),\n",
    "#       'targets': torch.tensor(label, dtype=torch.long)\n",
    "#     }\n",
    "# output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from transformers import BertModel\n",
    "# class BertClassifier(nn.Module):\n",
    "#     def __init__(self, dropout=0.5):\n",
    "#         super(BertClassifier, self).__init__()\n",
    "#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.linear = nn.Linear(768, 2)\n",
    "#         self.relu = nn.ReLU()\n",
    "#\n",
    "#     def forward(self, input_id, mask):\n",
    "#         _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "#         dropout_output = self.dropout(pooled_output)\n",
    "#         linear_output = self.linear(dropout_output)\n",
    "#         final_layer = self.relu(linear_output)\n",
    "#         return final_layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julinamaharjan/PycharmProjects/github-crawler/env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "optimizer = AdamW(bert_model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "# loss_fn = nn.CrossEntropyLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def train_epoch(\n",
    "  model,\n",
    "  data_loader,\n",
    "  loss_fn,\n",
    "  optimizer,\n",
    "  device,\n",
    "  scheduler,\n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs.logits, dim=1)\n",
    "    loss = loss_fn(outputs.logits, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs.logits, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julinamaharjan/PycharmProjects/github-crawler/env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:12\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "Input \u001B[0;32mIn [104]\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001B[0m\n\u001B[1;32m     29\u001B[0m correct_predictions \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(preds \u001B[38;5;241m==\u001B[39m targets)\n\u001B[1;32m     30\u001B[0m losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m---> 32\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m nn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), max_norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m)\n\u001B[1;32m     34\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/PycharmProjects/github-crawler/env/lib/python3.9/site-packages/torch/_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/github-crawler/env/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "history = defaultdict(list)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    bert_model,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(df_train)\n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    bert_model,\n",
    "    val_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(df_val)\n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(bert_model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}