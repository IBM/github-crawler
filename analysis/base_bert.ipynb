{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    " #!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from utils.cloudant_utils import cloudant_db as db\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['repo', 'release_tag', 'release_date', 'downloads', 'stars', 'watchers', 'forks', 'commits', 'issues', 'total_stars', 'total_forks', 'total_commits', 'contributors', 'total_issues', 'total_closedIssues', 'closedIssues', 'readme', 'readme_size'])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos = [r for r in db.get_query_result({\"type\": \"release\"}, [\"_id\", \"releases\"], limit=10000, raw_result=True)[\"docs\"]]\n",
    "repos[0]['releases'][0].keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "values = [r for release in repos for r in release[\"releases\"]]\n",
    "df = pd.DataFrame(values)\n",
    "df['contributors'] = df['contributors'].apply(lambda x:\n",
    "                                              [i for i in x if i is not None] if isinstance(x, list)\n",
    "                                              else [])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(44043, 19)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df = df[~df['readme'].isnull()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "new_df = df.groupby(\"repo\").agg({\"readme\": list,\n",
    "                                 \"stars\": sum,\n",
    "                                 \"forks\": sum,\n",
    "                                 \"downloads\": sum,\n",
    "                                 \"contributors\": sum})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "readme1 =  new_df.iloc[2]['readme'][0]\n",
    "readme2 = new_df.iloc[2]['readme'][1]\n",
    "# print(len(new_df))\n",
    "# new_df[new_df['readme'].map(lambda d: len(d)) > 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "new_df['600stars']= np.where(new_df['stars'] > 600, 1, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                           readme  \\\nrepo                                                                                \nDerwenAI/pytextrank             [# PyTextRank\\n\\n**PyTextRank** is a Python im...   \nultralytics/yolov3              [<img src=\"https://storage.googleapis.com/ultr...   \nramitsurana/awesome-kubernetes  [# Awesome-kubernetes [![Awesome](https://cdn....   \nquantumblacklabs/causalnex      [![CausalNex](docs/source/causalnex_banner.png...   \nfacebookresearch/mmf            [# Pythia\\n\\n[![Documentation Status](https://...   \n\n                                600stars  \nrepo                                      \nDerwenAI/pytextrank                    1  \nultralytics/yolov3                     1  \nramitsurana/awesome-kubernetes         1  \nquantumblacklabs/causalnex             1  \nfacebookresearch/mmf                   1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>readme</th>\n      <th>600stars</th>\n    </tr>\n    <tr>\n      <th>repo</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>DerwenAI/pytextrank</th>\n      <td>[# PyTextRank\\n\\n**PyTextRank** is a Python im...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>ultralytics/yolov3</th>\n      <td>[&lt;img src=\"https://storage.googleapis.com/ultr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>ramitsurana/awesome-kubernetes</th>\n      <td>[# Awesome-kubernetes [![Awesome](https://cdn....</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>quantumblacklabs/causalnex</th>\n      <td>[![CausalNex](docs/source/causalnex_banner.png...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>facebookresearch/mmf</th>\n      <td>[# Pythia\\n\\n[![Documentation Status](https://...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.loc[new_df['600stars'] == 1].sample(5)[['readme', '600stars']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    " # max_len = 0\n",
    "\n",
    " #for sentences in readme:\n",
    " #    if sentences:\n",
    " #        for sent in sentences:\n",
    " #            if sent:\n",
    " #                input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    " #                max_len = max(max_len, len(input_ids))\n",
    " #print('Max sentence length: ', max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import difflib\n",
    "def diff_calculator(str1, str2):\n",
    "   s = difflib.SequenceMatcher(lambda x : x == '')\n",
    "   s.set_seqs(str1, str2)\n",
    "   i = 1\n",
    "   codes = []\n",
    "   delete = []\n",
    "   replace = {}\n",
    "   insert = []\n",
    "   for (opcode, before_start, before_end, after_start, after_end) in s.get_opcodes():\n",
    "       if opcode == 'equal':\n",
    "           continue\n",
    "       codes.append(opcode)\n",
    "       # print (i, \". %7s '%s :'  ----->  '%s'\" % (opcode, test[0][before_start:before_end], test[1][after_start:after_end]))\n",
    "       if opcode == 'replace':\n",
    "           replace[str1[before_start:before_end]]  = str2[after_start:after_end]\n",
    "       if opcode == 'delete':\n",
    "           delete.append(str1[before_start:before_end])\n",
    "       if opcode == 'insert':\n",
    "           insert.append(str2[after_start:after_end])\n",
    "       i = i + 1\n",
    "   return replace, delete, insert"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# import re\n",
    "# def clean(str):\n",
    "#    return re.sub('\\s+', ' ', str) if str is not None else ''\n",
    "# i = 1\n",
    "# replace, _, _ = diff_calculator('haha', \"HAHA\")\n",
    "# for e in replace.keys():\n",
    "#    print(i, '. ', clean(e), ' -->', clean(replace[e]))\n",
    "#    i = i + 1\n",
    "   # print(e, ' -->', (replace[e]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def create_a_sequence(readmeList):\n",
    "   result = []\n",
    "   for i in range(0,len(readmeList)-1):\n",
    "       first = readmeList[i]\n",
    "       second = readmeList[i+1]\n",
    "       _, _, insert = diff_calculator(first, second)\n",
    "       result.append(','.join(insert))\n",
    "   return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# new_df['readme_diff'] = new_df['readme'].apply(lambda x: create_a_sequence(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'1.'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = create_a_sequence(new_df.iloc[2]['readme'])\n",
    "result[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def prepareSequenceForBERT(readmeList):\n",
    "    diffList = create_a_sequence(readmeList)\n",
    "    s = '[CLS]' + \"SEP\".join([str(i) for i in diffList])\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "((4151, 6), (231, 6), (231, 6))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df_train, df_test = train_test_split(new_df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "df_train.shape, df_val.shape, df_test.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ReadmeDataSet(Dataset):\n",
    "   def __init__(self, df, tokenizer, max_len):\n",
    "      self.df = df\n",
    "      self.tokenizer = tokenizer\n",
    "      self.max_len = max_len\n",
    "\n",
    "   def __len__(self):\n",
    "      return len(self.df)\n",
    "\n",
    "   def __getitem__(self, item):\n",
    "      sequence = prepareSequenceForBERT(self.df.iloc[item]['readme'][0])\n",
    "      # readmes = self.df.iloc[item]['readme']\n",
    "      # for i in range(0,len(readmes)-1):\n",
    "      #   first = readmes[i]\n",
    "      #   second = readmes[i+1]\n",
    "      #   _, _, insert = diff_calculator(first, second)\n",
    "      #   seq = ','.join(insert)\n",
    "      #\n",
    "      # sequence = '[CLS]' + \"SEP\".join([str(i) for i in diffList])\n",
    "\n",
    "      # sequence = \"Hi\"\n",
    "      target = self.df.iloc[item]['600stars']\n",
    "      encoding = self.tokenizer.encode_plus(sequence,\n",
    "                                     None,\n",
    "                                     max_length = self.max_len,\n",
    "                                     truncation=True,\n",
    "                                     add_special_tokens=True,\n",
    "#                                      padding=MAX_LEN,\n",
    "#                                      padding='longest',\n",
    "                                     pad_to_max_length=True,\n",
    "                                     return_token_type_ids=True)\n",
    "\n",
    "      return {\n",
    "      'sequence': sequence,\n",
    "      'input_ids': torch.tensor(encoding.input_ids, dtype=torch.long),\n",
    "      'attention_mask':  torch.tensor(encoding.attention_mask, dtype=torch.long),\n",
    "      'token_type_ids': torch.tensor(encoding.token_type_ids, dtype=torch.long),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "      }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "   ds = ReadmeDataSet(\n",
    "      df = df,\n",
    "      tokenizer=tokenizer,\n",
    "      max_len=max_len\n",
    "   )\n",
    "   return DataLoader(\n",
    "      ds,\n",
    "      batch_size=batch_size,\n",
    "      num_workers=0\n",
    "  )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import BERT Tokenizer and BERT Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                      num_labels=2,\n",
    "                                                      output_attentions= False,\n",
    "                                                      output_hidden_states= False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2 #16\n",
    "MAX_LEN = 100\n",
    "train_data_loader = create_data_loader(df_train, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, bert_tokenizer, MAX_LEN, BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# for d in train_data_loader:\n",
    "#     input_ids = d[\"input_ids\"].to(device)\n",
    "#     attention_mask = d[\"attention_mask\"].to(device)\n",
    "#     targets = d[\"targets\"].to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#TEST the tokenizer and data loader\n",
    "# sequence = prepareSequenceForBERT(new_df.iloc[2]['readme'])\n",
    "# label = new_df.iloc[2]['600stars']\n",
    "# tokens = bert_tokenizer.encode_plus(\n",
    "#             sequence,\n",
    "#             None,\n",
    "#             max_length= 100,\n",
    "#             truncation=True,\n",
    "#             add_special_tokens=True,\n",
    "# #             pad_to_max_length=True,\n",
    "#             padding = True,\n",
    "#             return_token_type_ids=True\n",
    "#         )\n",
    "# print(f' Sentence: {sequence}')\n",
    "# print(f' Tokens: {tokens}')\n",
    "# print(f' Tokens.token_type_ids: {tokens.token_type_ids}')\n",
    "# print(f' Tokens.input_ids: {len(tokens.input_ids)}')\n",
    "# output = {\n",
    "#       'input_ids': torch.tensor(tokens.input_ids, dtype=torch.long),\n",
    "#       'attention_mask':  torch.tensor(tokens.attention_mask, dtype=torch.long),\n",
    "#       'token_type_ids': torch.tensor(tokens.token_type_ids, dtype=torch.long),\n",
    "#       'targets': torch.tensor(label, dtype=torch.long)\n",
    "#     }\n",
    "# output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from transformers import BertModel\n",
    "# class BertClassifier(nn.Module):\n",
    "#     def __init__(self, dropout=0.5):\n",
    "#         super(BertClassifier, self).__init__()\n",
    "#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.linear = nn.Linear(768, 2)\n",
    "#         self.relu = nn.ReLU()\n",
    "#\n",
    "#     def forward(self, input_id, mask):\n",
    "#         _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "#         dropout_output = self.dropout(pooled_output)\n",
    "#         linear_output = self.linear(dropout_output)\n",
    "#         final_layer = self.relu(linear_output)\n",
    "#         return final_layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julinamaharjan/PycharmProjects/github-crawler/env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2#10\n",
    "optimizer = AdamW(bert_model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "# loss_fn = nn.CrossEntropyLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def train_epoch(\n",
    "  model,\n",
    "  data_loader,\n",
    "  loss_fn,\n",
    "  optimizer,\n",
    "  device,\n",
    "  scheduler,\n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs.logits, dim=1)\n",
    "    loss = loss_fn(outputs.logits, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs.logits, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julinamaharjan/PycharmProjects/github-crawler/env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.4813493224393757 accuracy 0.9079739821729703\n",
      "Val   loss 0.561400465032985 accuracy 0.8961038961038961\n",
      "\n",
      "Epoch 2/2\n",
      "----------\n",
      "Train loss 0.4741432095939735 accuracy 0.9084557937846303\n",
      "Val   loss 0.5387659213577943 accuracy 0.8961038961038961\n",
      "\n",
      "CPU times: user 4h 34min 46s, sys: 2min 53s, total: 4h 37min 39s\n",
      "Wall time: 3h 41min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "history = defaultdict(list)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    bert_model,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(df_train)\n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    bert_model,\n",
    "    val_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(df_val)\n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(bert_model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.8961, dtype=torch.float64)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julinamaharjan/PycharmProjects/github-crawler/env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.9177489177489178"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  bert_model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "\n",
    "  sequences = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"sequence\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "      _, preds = torch.max(outputs.logits, dim=1)\n",
    "      probs = F.softmax(outputs.logits, dim=1)\n",
    "\n",
    "      sequences.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return sequences, predictions, prediction_probs, real_values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julinamaharjan/PycharmProjects/github-crawler/env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_sequences, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  bert_model,\n",
    "  test_data_loader\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "a = y_test.numpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xq/pylr4t8d08g_cywhg3hhf02w0000gn/T/ipykernel_10233/4165259423.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  npa = np.asarray(someListOfLists)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                 tweet       Real  Predicted  \\\n0    [CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...  tensor(0)  tensor(0)   \n1    [CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...  tensor(0)  tensor(0)   \n2    [CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...  tensor(0)  tensor(0)   \n3    [CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...  tensor(0)  tensor(0)   \n4    [CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...  tensor(0)  tensor(0)   \n..                                                 ...        ...        ...   \n226  [CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...  tensor(0)  tensor(0)   \n227  [CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...  tensor(0)  tensor(0)   \n228  [CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...  tensor(0)  tensor(0)   \n229  [CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...  tensor(0)  tensor(0)   \n230  [CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...  tensor(0)  tensor(0)   \n\n                            Pred-prob  \n0    [tensor(0.9942), tensor(0.0058)]  \n1    [tensor(0.9942), tensor(0.0058)]  \n2    [tensor(0.9942), tensor(0.0058)]  \n3    [tensor(0.9942), tensor(0.0058)]  \n4    [tensor(0.9942), tensor(0.0058)]  \n..                                ...  \n226  [tensor(0.9942), tensor(0.0058)]  \n227  [tensor(0.9942), tensor(0.0058)]  \n228  [tensor(0.9942), tensor(0.0058)]  \n229  [tensor(0.9942), tensor(0.0058)]  \n230  [tensor(0.9942), tensor(0.0058)]  \n\n[231 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>Real</th>\n      <th>Predicted</th>\n      <th>Pred-prob</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...</td>\n      <td>tensor(0)</td>\n      <td>tensor(0)</td>\n      <td>[tensor(0.9942), tensor(0.0058)]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...</td>\n      <td>tensor(0)</td>\n      <td>tensor(0)</td>\n      <td>[tensor(0.9942), tensor(0.0058)]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...</td>\n      <td>tensor(0)</td>\n      <td>tensor(0)</td>\n      <td>[tensor(0.9942), tensor(0.0058)]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...</td>\n      <td>tensor(0)</td>\n      <td>tensor(0)</td>\n      <td>[tensor(0.9942), tensor(0.0058)]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...</td>\n      <td>tensor(0)</td>\n      <td>tensor(0)</td>\n      <td>[tensor(0.9942), tensor(0.0058)]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>226</th>\n      <td>[CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...</td>\n      <td>tensor(0)</td>\n      <td>tensor(0)</td>\n      <td>[tensor(0.9942), tensor(0.0058)]</td>\n    </tr>\n    <tr>\n      <th>227</th>\n      <td>[CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...</td>\n      <td>tensor(0)</td>\n      <td>tensor(0)</td>\n      <td>[tensor(0.9942), tensor(0.0058)]</td>\n    </tr>\n    <tr>\n      <th>228</th>\n      <td>[CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...</td>\n      <td>tensor(0)</td>\n      <td>tensor(0)</td>\n      <td>[tensor(0.9942), tensor(0.0058)]</td>\n    </tr>\n    <tr>\n      <th>229</th>\n      <td>[CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...</td>\n      <td>tensor(0)</td>\n      <td>tensor(0)</td>\n      <td>[tensor(0.9942), tensor(0.0058)]</td>\n    </tr>\n    <tr>\n      <th>230</th>\n      <td>[CLS]SEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSEPSE...</td>\n      <td>tensor(0)</td>\n      <td>tensor(0)</td>\n      <td>[tensor(0.9942), tensor(0.0058)]</td>\n    </tr>\n  </tbody>\n</table>\n<p>231 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "someListOfLists = list(zip(y_sequences, y_test, y_pred, y_pred_probs ))\n",
    "npa = np.asarray(someListOfLists)\n",
    "dff = pd.DataFrame(someListOfLists, columns = ['tweet', 'Real', 'Predicted', 'Pred-prob'])\n",
    "# dff['Real']= pd.to_numeric(df[\"Real\"])\n",
    "dff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/julinamaharjan/PycharmProjects/github-crawler/env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max() received an invalid combination of arguments - got (SequenceClassifierOutput, dim=int), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [44]\u001B[0m, in \u001B[0;36m<cell line: 17>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     13\u001B[0m token_type_ids \u001B[38;5;241m=\u001B[39m encoded_tweet[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     16\u001B[0m output \u001B[38;5;241m=\u001B[39m bert_model(input_ids, attention_mask, token_type_ids)\n\u001B[0;32m---> 17\u001B[0m _, prediction \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mREADME text: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreadme\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrediction  : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprediction\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: max() received an invalid combination of arguments - got (SequenceClassifierOutput, dim=int), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n"
     ]
    }
   ],
   "source": [
    "readme = \"\"\n",
    "encoded_tweet = bert_tokenizer.encode_plus(\n",
    "  readme,\n",
    "  max_length=MAX_LEN,\n",
    "  add_special_tokens=True,\n",
    "  return_token_type_ids=True,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")\n",
    "input_ids = encoded_tweet['input_ids'].to(device)\n",
    "attention_mask = encoded_tweet['attention_mask'].to(device)\n",
    "token_type_ids = encoded_tweet['token_type_ids'].to(device)\n",
    "\n",
    "\n",
    "output = bert_model(input_ids, attention_mask, token_type_ids)\n",
    "_, prediction = torch.max(output, dim=1)\n",
    "print(f'README text: {readme}')\n",
    "print(f'Prediction  : {prediction}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}