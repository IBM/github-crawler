{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    " #!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from utils.cloudant_utils import cloudant_db as db\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['repo', 'release_tag', 'release_date', 'downloads', 'stars', 'watchers', 'forks', 'commits', 'issues', 'total_stars', 'total_forks', 'total_commits', 'contributors', 'total_issues', 'total_closedIssues', 'closedIssues', 'readme', 'readme_size'])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos = [r for r in db.get_query_result({\"type\": \"release\"}, [\"_id\", \"releases\"], limit=10000, raw_result=True)[\"docs\"]]\n",
    "repos[0]['releases'][0].keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "values = [r for release in repos for r in release[\"releases\"]]\n",
    "df = pd.DataFrame(values)\n",
    "df['contributors'] = df['contributors'].apply(lambda x:\n",
    "                                              [i for i in x if i is not None] if isinstance(x, list)\n",
    "                                              else [])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "new_df = df.groupby(\"repo\").agg({\"readme\": list,\n",
    "                                 \"stars\": sum,\n",
    "                                 \"forks\": sum,\n",
    "                                 \"downloads\": sum,\n",
    "                                 \"contributors\": sum})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "'## Klasifai\\n\\nClassify WordPress Content using [IBM Watson Natural Language Processing API](https://www.ibm.com/watson/services/natural-language-understanding/).\\n\\n[![Build Status](https://travis-ci.org/10up/klasifai.svg?branch=master)](https://travis-ci.org/10up/klasifai)\\n\\n## Features\\n\\n* Classify Post Content using IBM Watson NLU API\\n* Supports Watson Categories, Keywords, Concepts & Entities\\n* Bulk Classify Posts\\n* Automatically classify content on save\\n\\n## Installation\\n\\n#### 1. Download or Clone this repo\\n\\n#### 2. Activate Plugin\\n\\n#### 3. Configure IBM Watson API Keys under Settings > Klasifai\\n\\n#### 4. Configure Post Types unde Settings > Klasifai\\n\\n#### 5. Save Post or run WP CLI command to batch classify posts\\n\\n## WP CLI\\n\\n#### 1. Batch Classify Posts\\n\\n$ wp klasifai post {post_ids} [--post_type=post_type] [--limit=limit] [--link=link]\\n\\n[--post_type=post_type]\\n    Batch classify posts belonging to this post type. If false\\n    relies on post_ids in args\\n    ---\\n    default: false\\n    options:\\n      - any other post type name\\n      - false, if args contains post_ids\\n    ---\\n\\n  [--limit=limit]\\n    Limit classification to N posts.\\n    ---\\n    default: false\\n    options:\\n      - false, no limit\\n      - N, max number of posts to classify\\n    ---\\n\\n  [--link=link]\\n    Whether to link classification results to Taxonomy terms\\n    ---\\n    default: true\\n    options:\\n      - bool, any bool value\\n    ---\\n\\n#### 2. Classify Text\\n\\nwp klasifai text {text} [--category=bool] [--keyword=bool] [--concept=bool] [--entity=bool] [--input=input] [--only-normalize=bool]\\n\\nDirectly classify text using Watson NLU.\\n\\nOptions\\n\\n  [--category=bool]\\n    Enables NLU category feature\\n    ---\\n    default: true\\n    options:\\n      - any boolean value\\n    ---\\n\\n  [--keyword=bool]\\n    Enables NLU keyword feature\\n    ---\\n    default: true\\n    options:\\n      - any boolean value\\n    ---\\n\\n  [--concept=bool]\\n    Enables NLU concept feature\\n    ---\\n    default: true\\n    options:\\n      - any boolean value\\n    ---\\n\\n  [--entity=bool]\\n    Enables NLU entity feature\\n    ---\\n    default: true\\n    options:\\n      - any boolean value\\n    ---\\n\\n  [--input=input]\\n    Path to input file or URL\\n    ---\\n    default: false\\n    options:\\n      - path to local file\\n      - path to remote URL\\n      - false, uses args[0] instead\\n    ---\\n\\n  [--only-normalize=<bool>]\\n    Prints the normalized text that will be sent to the NLU API\\n    ---\\n    default: false\\n    options:\\n      - any boolean value\\n    ---\\n\\n## Contributing\\n\\n1. Clone the repo\\n2. Create Pull Request against the master branch.\\n3. Fix failing tests if any.\\n\\n## License\\n\\nKlasifai is free software; you can redistribute it and/or modify it\\nunder the terms of the GNU General Public License as published by the\\nFree Software Foundation; either version 2 of the License, or (at your\\noption) any later version.\\n'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.iloc[2]['readme'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "new_df['600stars']= np.where(new_df['stars'] > 600, 1, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                          readme  \\\nrepo                                                                               \nalan-turing-institute/MLJ.jl   [## MLJ\\n\\nA pure Julia machine learning frame...   \nYunYang1994/tensorflow-yolov3  [## part 1. Introduction\\n\\nImplementation of ...   \naamini/introtodeeplearning     [# MIT 6.S191: Introduction to Deep Learning\\n...   \ncoqui-ai/TTS                   [# <img src=\"images/coqui-log-green-TTS.png\" h...   \nEpistasisLab/tpot              [[![Build Status](https://travis-ci.org/rhieve...   \n\n                               600stars  \nrepo                                     \nalan-turing-institute/MLJ.jl          1  \nYunYang1994/tensorflow-yolov3         1  \naamini/introtodeeplearning            1  \ncoqui-ai/TTS                          1  \nEpistasisLab/tpot                     1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>readme</th>\n      <th>600stars</th>\n    </tr>\n    <tr>\n      <th>repo</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alan-turing-institute/MLJ.jl</th>\n      <td>[## MLJ\\n\\nA pure Julia machine learning frame...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>YunYang1994/tensorflow-yolov3</th>\n      <td>[## part 1. Introduction\\n\\nImplementation of ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>aamini/introtodeeplearning</th>\n      <td>[# MIT 6.S191: Introduction to Deep Learning\\n...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>coqui-ai/TTS</th>\n      <td>[# &lt;img src=\"images/coqui-log-green-TTS.png\" h...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>EpistasisLab/tpot</th>\n      <td>[[![Build Status](https://travis-ci.org/rhieve...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.loc[new_df['600stars'] == 1].sample(5)[['readme', '600stars']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " # max_len = 0\n",
    "\n",
    " #for sentences in readme:\n",
    " #    if sentences:\n",
    " #        for sent in sentences:\n",
    " #            if sent:\n",
    " #                input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    " #                max_len = max(max_len, len(input_ids))\n",
    " #print('Max sentence length: ', max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import difflib\n",
    "def diff_calculator(str1, str2):\n",
    "   s = difflib.SequenceMatcher(lambda x : x == '')\n",
    "   s.set_seqs(str1, str2)\n",
    "   i = 1\n",
    "   codes = []\n",
    "   delete = []\n",
    "   replace = {}\n",
    "   insert = []\n",
    "   for (opcode, before_start, before_end, after_start, after_end) in s.get_opcodes():\n",
    "       if opcode == 'equal':\n",
    "           continue\n",
    "       codes.append(opcode)\n",
    "       # print (i, \". %7s '%s :'  ----->  '%s'\" % (opcode, test[0][before_start:before_end], test[1][after_start:after_end]))\n",
    "       if opcode == 'replace':\n",
    "           replace[str1[before_start:before_end]]  = str2[after_start:after_end]\n",
    "       if opcode == 'delete':\n",
    "           delete.append(str1[before_start:before_end])\n",
    "       if opcode == 'insert':\n",
    "           insert.append(str2[after_start:after_end])\n",
    "       i = i + 1\n",
    "   return replace, delete, insert"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 .  haha  --> HAHA\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# def clean(str):\n",
    "#    return re.sub('\\s+', ' ', str) if str is not None else ''\n",
    "# i = 1\n",
    "# replace, _, _ = diff_calculator('haha', \"HAHA\")\n",
    "# for e in replace.keys():\n",
    "#    print(i, '. ', clean(e), ' -->', clean(replace[e]))\n",
    "#    i = i + 1\n",
    "   # print(e, ' -->', (replace[e]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def create_a_sequence(readmeList):\n",
    "   result = []\n",
    "   for i in range(0,len(readmeList)-1):\n",
    "       first = readmeList[i]\n",
    "       second = readmeList[i+1]\n",
    "       _, _, insert = diff_calculator(first, second)\n",
    "       result.append(insert)\n",
    "   return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# new_df['readme_diff'] = new_df['readme'].apply(lambda x: create_a_sequence(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "((4493, 6), (250, 6), (250, 6))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df_train, df_test = train_test_split(new_df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "df_train.shape, df_val.shape, df_test.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ReadmeDataSet(Dataset):\n",
    "   def __init__(self, sequence, targets, tokenizer, max_len):\n",
    "      self.sequence = sequence\n",
    "      self.targets = targets\n",
    "      self.tokenizer = tokenizer\n",
    "      self.max_len = max_len\n",
    "\n",
    "   def __len__(self):\n",
    "      return len(self.sequence)\n",
    "\n",
    "   def __getitem__(self, item):\n",
    "      sequence = str(self.sequence[item])\n",
    "      target = self.targets[item]\n",
    "      encoding = self.tokenizer.encode_plus(sequence,\n",
    "                                     None,\n",
    "                                     max_length = self.max_len,\n",
    "                                     truncation=True,\n",
    "                                     add_special_tokens=True,\n",
    "#                                      padding=MAX_LEN,\n",
    "#                                      padding='longest',\n",
    "                                     pad_to_max_length=True,\n",
    "                                     return_token_type_ids=True)\n",
    "\n",
    "      return {\n",
    "      'sequence': sequence,\n",
    "      'input_ids': torch.tensor(encoding.input_ids, dtype=torch.long),\n",
    "      'attention_mask':  torch.tensor(encoding.attention_mask, dtype=torch.long),\n",
    "      'token_type_ids': torch.tensor(encoding.token_type_ids, dtype=torch.long),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "      }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Tokenizer . . .\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "print('Loading BERT Tokenizer . . .')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                      num_labels=2,\n",
    "                                                      output_attentions= False,\n",
    "                                                      output_hidden_states= False)\n",
    "# model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentence: This is awesome. We are almost there!\n",
      " Tokens: {'input_ids': [101, 2023, 2003, 12476, 1012, 2057, 2024, 2471, 2045, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      " Tokens.token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " Tokens.input_ids: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input_ids': tensor([  101,  2023,  2003, 12476,  1012,  2057,  2024,  2471,  2045,   999,\n           102]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 'targets': tensor(1)}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = 'This is awesome. We are almost there!'\n",
    "label = 1\n",
    "tokens = bert_tokenizer.encode_plus(\n",
    "            sample_text,\n",
    "            None,\n",
    "            max_length= 100,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "#             pad_to_max_length=True,\n",
    "            padding = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "print(f' Sentence: {sample_text}')\n",
    "print(f' Tokens: {tokens}')\n",
    "print(f' Tokens.token_type_ids: {tokens.token_type_ids}')\n",
    "print(f' Tokens.input_ids: {len(tokens.input_ids)}')\n",
    "output = {\n",
    "      'input_ids': torch.tensor(tokens.input_ids, dtype=torch.long),\n",
    "      'attention_mask':  torch.tensor(tokens.attention_mask, dtype=torch.long),\n",
    "      'token_type_ids': torch.tensor(tokens.token_type_ids, dtype=torch.long),\n",
    "      'targets': torch.tensor(label, dtype=torch.long)\n",
    "    }\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "   ds = ReadmeDataSet(\n",
    "      sequence=df.readme.to_numpy(),\n",
    "      targets=df['600stars'].to_numpy(),\n",
    "      tokenizer=tokenizer,\n",
    "      max_len=max_len\n",
    "   )\n",
    "   return DataLoader(\n",
    "      ds,\n",
    "      batch_size=batch_size,\n",
    "      num_workers=4\n",
    "  )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m BATCH_SIZE \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m16\u001B[39m\n\u001B[0;32m----> 3\u001B[0m train_data_loader \u001B[38;5;241m=\u001B[39m create_data_loader(df_train, bert_tokenizer, \u001B[43mMAX_LEN\u001B[49m, BATCH_SIZE)\n\u001B[1;32m      4\u001B[0m val_data_loader \u001B[38;5;241m=\u001B[39m create_data_loader(df_val, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n\u001B[1;32m      5\u001B[0m test_data_loader \u001B[38;5;241m=\u001B[39m create_data_loader(df_test, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'MAX_LEN' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 28000\n",
    "train_data_loader = create_data_loader(df_train, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, bert_tokenizer, MAX_LEN, BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}