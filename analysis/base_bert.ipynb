{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    " #!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS connecting to Cloudant db github-public-ai-2022\n"
     ]
    }
   ],
   "source": [
    "from utils.cloudant_utils import cloudant_db as db\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['repo', 'release_tag', 'release_date', 'downloads', 'stars', 'watchers', 'forks', 'commits', 'issues', 'total_stars', 'total_forks', 'total_commits', 'contributors', 'total_issues', 'total_closedIssues', 'closedIssues', 'readme', 'readme_size'])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos = [r for r in db.get_query_result({\"type\": \"release\"}, [\"_id\", \"releases\"], limit=10000, raw_result=True)[\"docs\"]]\n",
    "repos[0]['releases'][0].keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "values = [r for release in repos for r in release[\"releases\"]]\n",
    "df = pd.DataFrame(values)\n",
    "df['contributors'] = df['contributors'].apply(lambda x:\n",
    "                                              [i for i in x if i is not None] if isinstance(x, list)\n",
    "                                              else [])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "new_df = df.groupby(\"repo\").agg({\"readme\": list,\n",
    "                                 \"stars\": sum,\n",
    "                                 \"forks\": sum,\n",
    "                                 \"downloads\": sum,\n",
    "                                 \"contributors\": sum})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "readme1 =  new_df.iloc[2]['readme'][0]\n",
    "readme2 = new_df.iloc[2]['readme'][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "new_df['600stars']= np.where(new_df['stars'] > 600, 1, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                 readme  \\\nrepo                                                                      \nNVIDIA/NeMo           [None, None, None, None, None, None, None, Non...   \nAngel-ML/angel        [![](assets/angel_logo.png)\\n\\n\\n[![license](h...   \nbentoml/BentoML       [None, None, None, None, None, None, None, Non...   \nBlazingDB/blazingsql  [![](http://www.blazingdb.com/images/Logo_Blaz...   \nuber/neuropod         [None, None, None, docs/index.md, docs/index.m...   \n\n                      600stars  \nrepo                            \nNVIDIA/NeMo                  1  \nAngel-ML/angel               1  \nbentoml/BentoML              1  \nBlazingDB/blazingsql         1  \nuber/neuropod                1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>readme</th>\n      <th>600stars</th>\n    </tr>\n    <tr>\n      <th>repo</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>NVIDIA/NeMo</th>\n      <td>[None, None, None, None, None, None, None, Non...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Angel-ML/angel</th>\n      <td>[![](assets/angel_logo.png)\\n\\n\\n[![license](h...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>bentoml/BentoML</th>\n      <td>[None, None, None, None, None, None, None, Non...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>BlazingDB/blazingsql</th>\n      <td>[![](http://www.blazingdb.com/images/Logo_Blaz...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>uber/neuropod</th>\n      <td>[None, None, None, docs/index.md, docs/index.m...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.loc[new_df['600stars'] == 1].sample(5)[['readme', '600stars']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " # max_len = 0\n",
    "\n",
    " #for sentences in readme:\n",
    " #    if sentences:\n",
    " #        for sent in sentences:\n",
    " #            if sent:\n",
    " #                input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    " #                max_len = max(max_len, len(input_ids))\n",
    " #print('Max sentence length: ', max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import difflib\n",
    "def diff_calculator(str1, str2):\n",
    "   s = difflib.SequenceMatcher(lambda x : x == '')\n",
    "   s.set_seqs(str1, str2)\n",
    "   i = 1\n",
    "   codes = []\n",
    "   delete = []\n",
    "   replace = {}\n",
    "   insert = []\n",
    "   for (opcode, before_start, before_end, after_start, after_end) in s.get_opcodes():\n",
    "       if opcode == 'equal':\n",
    "           continue\n",
    "       codes.append(opcode)\n",
    "       # print (i, \". %7s '%s :'  ----->  '%s'\" % (opcode, test[0][before_start:before_end], test[1][after_start:after_end]))\n",
    "       if opcode == 'replace':\n",
    "           replace[str1[before_start:before_end]]  = str2[after_start:after_end]\n",
    "       if opcode == 'delete':\n",
    "           delete.append(str1[before_start:before_end])\n",
    "       if opcode == 'insert':\n",
    "           insert.append(str2[after_start:after_end])\n",
    "       i = i + 1\n",
    "   return replace, delete, insert"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 .  haha  --> HAHA\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# def clean(str):\n",
    "#    return re.sub('\\s+', ' ', str) if str is not None else ''\n",
    "# i = 1\n",
    "# replace, _, _ = diff_calculator('haha', \"HAHA\")\n",
    "# for e in replace.keys():\n",
    "#    print(i, '. ', clean(e), ' -->', clean(replace[e]))\n",
    "#    i = i + 1\n",
    "   # print(e, ' -->', (replace[e]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def create_a_sequence(readmeList):\n",
    "   result = []\n",
    "   for i in range(0,len(readmeList)-1):\n",
    "       first = readmeList[i]\n",
    "       second = readmeList[i+1]\n",
    "       _, _, insert = diff_calculator(first, second)\n",
    "       result.append(','.join(insert))\n",
    "   return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# new_df['readme_diff'] = new_df['readme'].apply(lambda x: create_a_sequence(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "result = create_a_sequence(new_df.iloc[2]['readme'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'[,](https://wp-cli.org/)'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def prepareSequenceForBERT(readmeList):\n",
    "    diffList = create_a_sequence(readmeList)\n",
    "    s = '[CLS]' + \"SEP\".join([str(i) for i in diffList])\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "((4493, 6), (250, 6), (250, 6))"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df_train, df_test = train_test_split(new_df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "df_train.shape, df_val.shape, df_test.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ReadmeDataSet(Dataset):\n",
    "   def __init__(self, df, tokenizer, max_len):\n",
    "      self.df = df\n",
    "      self.tokenizer = tokenizer\n",
    "      self.max_len = max_len\n",
    "\n",
    "   def __len__(self):\n",
    "      return len(self.df)\n",
    "\n",
    "   def __getitem__(self, item):\n",
    "      sequence = prepareSequenceForBERT(self.df.iloc[item]['readme'])\n",
    "      target = self.df.iloc[item]['600stars']\n",
    "      encoding = self.tokenizer.encode_plus(sequence,\n",
    "                                     None,\n",
    "                                     max_length = self.max_len,\n",
    "                                     truncation=True,\n",
    "                                     add_special_tokens=True,\n",
    "#                                      padding=MAX_LEN,\n",
    "#                                      padding='longest',\n",
    "                                     pad_to_max_length=True,\n",
    "                                     return_token_type_ids=True)\n",
    "\n",
    "      return {\n",
    "      'sequence': sequence,\n",
    "      'input_ids': torch.tensor(encoding.input_ids, dtype=torch.long),\n",
    "      'attention_mask':  torch.tensor(encoding.attention_mask, dtype=torch.long),\n",
    "      'token_type_ids': torch.tensor(encoding.token_type_ids, dtype=torch.long),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "      }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT Tokenizer . . .\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "print('Loading BERT Tokenizer . . .')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                      num_labels=2,\n",
    "                                                      output_attentions= False,\n",
    "                                                      output_hidden_states= False)\n",
    "# model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentence: [CLS]'s, \\,Cloud account](https://cloud.ibm.com/registration) or sign into your existing one.\n",
      "- Check for an email from `IBM Cloud` and click the `Confirm Account` link.\n",
      "- Log into your account (accepting the privacy policy) and create a new [*Natural Language Understanding*](https://cloud.ibm.com/catalog/services/natural-language-understanding) Resource if you do not already have one. It may take a minute for your account to fully populate with the default resource group to use.\n",
      "- Click `Manage` in the left hand menu, then `Show credentials` on the Manage page to view the credentials for this resource.\n",
      "\n",
      "#### 4. Configure IBM Watson API Keys under Settings > ClassifAI\n",
      "\n",
      "**The credentials screen will show either an API key or a username/password combination.**\n",
      "\n",
      "##### If your credentials contain an API Key, then:\n",
      "- In the `API URL` field enter the URL\n",
      "- In the `API User` field, enter `apikey`.\n",
      "- Enter your API Key in the `API Key` field.\n",
      "\n",
      "##### If your credentials contain a username and password, then:\n",
      "- In the `API URL` field enter the URL\n",
      "- Enter the `username` value into the `API User field`.\n",
      "- Enter the `password` into the `API key` field.\n",
      "\n",
      "#### 5. Configure Post Types to classify and IBM Watson Features to enable under Settings > ClassifAI\n",
      "- Choose which public post types to classify when saved.\n",
      "- Chose whether to assign category, keyword, entity, and concept as well as the taxonomies used for each.\n",
      "\n",
      "#### 6. Save Post or run WP CLI command to batch classify posts\n",
      "\n",
      "## WP CLI Usage Instructions\n",
      "\n",
      "#### 1. Batch Classify Posts\n",
      "\n",
      "`$ wp klasifai post {post_ids} [--post_type=post_type] [--limit=limit] [--link=link]`\n",
      "\n",
      "##### Options\n",
      "\n",
      "`--post_type=post_type`\n",
      "\n",
      "Batch classify posts belonging to this post type. If `false` or absent relies on `post_ids` in args\n",
      "\n",
      "default: `false`    \n",
      "options:    \n",
      "- any post type name    \n",
      "- `false`, if args contains `post_ids`\n",
      "\n",
      "`--limit=limit`\n",
      "\n",
      "Limit classification to N posts.\n",
      "\n",
      "default: `false`    \n",
      "options:    \n",
      "- `false`, no limit    \n",
      "- `N`, max number of posts to classify\n",
      "\n",
      "`--link=link`\n",
      "\n",
      "Whether to link classification results to Taxonomy terms\n",
      "\n",
      "default: `true`\n",
      "\n",
      "#### 2. Classify Text\n",
      "\n",
      "`$ wp klasifai text {text} [--category=bool] [--keyword=bool] [--concept=bool] [--entity=bool] [--input=input] [--only-normalize=bool]`\n",
      "\n",
      "Directly classify text using ,.\n",
      "\n",
      "##### Options\n",
      "\n",
      "`--category=bool`\n",
      "\n",
      "Enables NLU category feature\n",
      "\n",
      "default: `true`    \n",
      "\n",
      "`--keyword=bool`\n",
      "\n",
      "Enables NLU keyword feature\n",
      "\n",
      "default: `true`    \n",
      "\n",
      "`--concept=bool`\n",
      "\n",
      "Enables NLU concept feature\n",
      "\n",
      "default: `true`\n",
      "\n",
      "`--entity=bool`\n",
      "\n",
      "Enables NLU entity feature\n",
      "\n",
      "default: `true`\n",
      "\n",
      "`--input=input`\n",
      "\n",
      "Path to input file or URL\n",
      "\n",
      "default: `false`    \n",
      "options:    \n",
      "- path to local file    \n",
      "- path to remote URL    \n",
      "- `false`, uses args[0] instead\n",
      "\n",
      "`--only-normalize=<bool>`\n",
      "\n",
      "Prints the normalized text that will be sent to the NLU,UTING.md](https://github.com/10up/classifai-for-wordpress/blob/develop/CONTRIBUTING.md) for details on the process for submitting pull requests to us.\n",
      "\n",
      "## License\n",
      "\n",
      "ClassifAI utilizes an [SEP[,](https://wp-cli.org/)SEP ![WordPress tested up to version](https://img.shields.io/badge/WordPress-v5.2%20tested-success.svg)\n",
      "\n",
      "## Table of Contents  \n",
      "* [Features](#features)\n",
      "* [Installation](#installation)\n",
      "* [Set Up Content Tagging](#set-up-content-tagging-via-ibm-watson)\n",
      "* [Set Up Image Processing](#set-up-image-processing-via-microsoft-azure)\n",
      "* [WP CLI Usage Instructions](#wp-cli-usage-instructions)\n",
      "* [Changelog](#changelog)\n",
      "* [Contributing](#contributing), and [Microsoft Azure's Computer Vision API](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/), and Azure's [Describe Image](https://westus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fe), and images, account](https://cloud.ibm.com/registration) or sign into your existing one.\n",
      "- Check for an email from `IBM Cloud, > Language Processing, > Language Processing,Set Up Image Processing (via Microsoft Azure)\n",
      "\n",
      "Note that [Computer Vision](https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/home#image-requirements) can analyze images that meet the following requirements:\n",
      "- The image must be presented in JPEG, PNG, GIF, or BMP format\n",
      "- The file size of the image must be less than 4 megabytes (MB)\n",
      "- The dimensions of the image must be greater than 50 x 50 pixels\n",
      "\n",
      "Note that Computer Vision has a [free pricing tier](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/computer-vision/) that offers 20 transactions per minute and 5,000 transactions per month.  \n",
      "\n",
      "#### 1. Sign up for Azure services\n",
      "- [Register for a Microsoft Azure account](https://azure.microsoft.com/en-us/free/) or sign into your existing one.\n",
      "- Log into your account and create a new [*Computer Vision*](https://portal.azure.com/#blade/Microsoft_Azure_Marketplace/GalleryFeaturedMenuItemBlade/selectedMenuItemId/CognitiveServices_MP/dontDiscardJourney/true/launchingContext/%7B%22source%22%3A%22Resources%20Microsoft.CognitiveServices%2Faccounts%22%7D/resetMenuId/) Service if you do not already have one.  It may take a minute for your account to fully populate with the default resource group to use.\n",
      "- Click `Quick start` in the left hand Resource Management menu to view the `API endpoint` credential for this resource in section `2b`.\n",
      "- Click `Keys` in the left hand Resource Management menu to view the `Key 1` credential for this resource.\n",
      "\n",
      "#### 2. Configure Microsoft Azure API and Key under ClassifAI > Image Processing\n",
      "- In the `Endpoint URL` field, enter your `API endpoint`.\n",
      "- In the `API Key` field, enter your `Key 1`.\n",
      "\n",
      "#### 3. Save Image to classify image\n",
      "\n",
      "## ,\n",
      "\n",
      "## Changelog\n",
      "\n",
      "A complete listing of all notable changes to Distributor are documented in [CHANGELOG.md](https://github.com/10up/classifai/blob/develop/CHANGELOG.md).SEP [![MIT License](https://img.shields.io/github/license/10up/classifai.svg)](https://github.com/10up/classifai/blob/develop/LICENSE.md),s)\n",
      "* [Requirements](#requirement,\n",
      "\n",
      "## Requirements\n",
      "\n",
      "* PHP 7.0+\n",
      "* [WordPress](http://wordpress.org) 5.0+\n",
      "* To utilize the Lanaguage Processing functionality, you will need an active [IBM Watson](https://cloud.ibm.com/registration) account.\n",
      "* To utilize the Image Processing functionality, you will need an active [Microsoft Azure](https://signup.azure.com/signup) account.SEP[![Support Level](https://img.shields.io/badge/support-active-green.svg)](#support-level) ,)\n",
      "* [Data Gathering](#data-gathering)\n",
      "* [Support](#support-level,\n",
      "\n",
      "## Data Gathering\n",
      "\n",
      "ClassifAI connects your WordPress site directly to your account with specific service provider(s) (e.g. Microsoft Azure AI, IBM Watson), so no data is gathered by 10up.  The data gathered in our [registration form](https://classifaiplugin.com/#cta) is used simply to stay in touch with users so we can provide product updates and news.  More information is available in the [Privacy Policy on ClassifAIplugin.com](https://drive.google.com/open?id=1Hn4XEWmNGqeMzLqnS7Uru2Hl2vJeLc7cI7225ztThgQ).\n",
      "\n",
      "## Support Level\n",
      "\n",
      "**Active:** 10up is actively working on this, and we expect to continue work for the foreseeable future including keeping tested up to the most recent version of WordPress.  Bug reports, feature requests, questions, and pull requests are welcome.SEP- The file must be externally accessible via URL (i.e. local sites and setups that block direct file access will not work out of the box)\n",
      ",\n",
      "`{ post_ids }`\n",
      "\n",
      "A comma delimited list of post ids to classify. Used if `post_type` is false or absent.\n",
      "\n",
      "default: `true`\n",
      "\n",
      "\n",
      "SEPManually generate alt text and image tags for images\n",
      "* [Smartly crop images](https://docs.microsoft.com/en-us/rest/api/cognitiveservices/computervision/generatethumbnail) around a region of interest identified by Computer Vision\n",
      "* ,Pricing\n",
      "\n",
      "Note that there is no cost to using ClassifA,lassifAI account\n",
      "- Register for a free ClassifAI account [here](https://classifaiplugin.com/#cta).\n",
      "- C,ClassifAI Team` which contains the registration key.\n",
      "- Note that the email will be sent from `opensource@10up.com`, so please whitelist this email address if needed.\n",
      "\n",
      "#### 2. Configure ClassifAI API Keys under admin area > ClassifAI\n",
      "- In the `Registered Email` field, enter the email you used for registration.\n",
      "- In the `Registration Key` field, enter the registration key from the email in step 1 above.\n",
      "\n",
      "## Set Up Language Processing (via ,Watson)\n",
      "\n",
      "#### 1. Sign up for Watson services\n",
      "- [Register for an IBM Cloud account](https://cloud.ibm.com/registration) or sign into your existing one.\n",
      "- Check for an email from `IBM ,o, and crop,## 3. Classify Image \n",
      "\n",
      "`$ wp classifai image {image_ids} [--limit=int] [--force]`\n",
      "\n",
      "Directly classify images using Azure Computer Vision.\n",
      "\n",
      "##### Options\n",
      "\n",
      "`--limit=int`\n",
      "\n",
      "Limit number of images to classify.\n",
      "\n",
      "default: `false`\n",
      "\n",
      "`--force`\n",
      "\n",
      "Force classifying images regardless of their `alt`.\n",
      "\n",
      "default: `false`\n",
      "\n",
      "##,, and [CREDITS.md](https://github.com/10up/classifai/blob/develop/CREDITS.md) for a listing of maintainers, contributors, and libraries for ClassifAISEPSEPSEP\n",
      " Tokens: {'input_ids': [101, 101, 1005, 1055, 1010, 1032, 1010, 6112, 4070, 1033, 1006, 16770, 1024, 1013, 1013, 6112, 1012, 9980, 1012, 4012, 1013, 8819, 1007, 2030, 3696, 2046, 2115, 4493, 2028, 1012, 1011, 4638, 2005, 2019, 10373, 2013, 1036, 9980, 6112, 1036, 1998, 11562, 1996, 1036, 12210, 4070, 1036, 4957, 1012, 1011, 8833, 2046, 2115, 4070, 1006, 10564, 1996, 9394, 3343, 1007, 1998, 3443, 1037, 2047, 1031, 1008, 3019, 2653, 4824, 1008, 1033, 1006, 16770, 1024, 1013, 1013, 6112, 1012, 9980, 1012, 4012, 1013, 12105, 1013, 2578, 1013, 3019, 1011, 2653, 1011, 4824, 1007, 7692, 2065, 2017, 2079, 2025, 2525, 2031, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      " Tokens.token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " Tokens.input_ids: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input_ids': tensor([  101,   101,  1005,  1055,  1010,  1032,  1010,  6112,  4070,  1033,\n          1006, 16770,  1024,  1013,  1013,  6112,  1012,  9980,  1012,  4012,\n          1013,  8819,  1007,  2030,  3696,  2046,  2115,  4493,  2028,  1012,\n          1011,  4638,  2005,  2019, 10373,  2013,  1036,  9980,  6112,  1036,\n          1998, 11562,  1996,  1036, 12210,  4070,  1036,  4957,  1012,  1011,\n          8833,  2046,  2115,  4070,  1006, 10564,  1996,  9394,  3343,  1007,\n          1998,  3443,  1037,  2047,  1031,  1008,  3019,  2653,  4824,  1008,\n          1033,  1006, 16770,  1024,  1013,  1013,  6112,  1012,  9980,  1012,\n          4012,  1013, 12105,  1013,  2578,  1013,  3019,  1011,  2653,  1011,\n          4824,  1007,  7692,  2065,  2017,  2079,  2025,  2525,  2031,   102]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0]),\n 'targets': tensor(0)}"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = prepareSequenceForBERT(new_df.iloc[2]['readme'])\n",
    "label = new_df.iloc[2]['600stars']\n",
    "tokens = bert_tokenizer.encode_plus(\n",
    "            sequence,\n",
    "            None,\n",
    "            max_length= 100,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "#             pad_to_max_length=True,\n",
    "            padding = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "print(f' Sentence: {sequence}')\n",
    "print(f' Tokens: {tokens}')\n",
    "print(f' Tokens.token_type_ids: {tokens.token_type_ids}')\n",
    "print(f' Tokens.input_ids: {len(tokens.input_ids)}')\n",
    "output = {\n",
    "      'input_ids': torch.tensor(tokens.input_ids, dtype=torch.long),\n",
    "      'attention_mask':  torch.tensor(tokens.attention_mask, dtype=torch.long),\n",
    "      'token_type_ids': torch.tensor(tokens.token_type_ids, dtype=torch.long),\n",
    "      'targets': torch.tensor(label, dtype=torch.long)\n",
    "    }\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "   ds = ReadmeDataSet(\n",
    "      df = df,\n",
    "      tokenizer=tokenizer,\n",
    "      max_len=max_len\n",
    "   )\n",
    "   return DataLoader(\n",
    "      ds,\n",
    "      batch_size=batch_size,\n",
    "      num_workers=4\n",
    "  )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2 #16\n",
    "MAX_LEN = 100\n",
    "train_data_loader = create_data_loader(df_train, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, bert_tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, bert_tokenizer, MAX_LEN, BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model,\n",
    "  data_loader,\n",
    "  loss_fn,\n",
    "  optimizer,\n",
    "  device,\n",
    "  scheduler,\n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(df_train)\n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(df_val)\n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}